{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file: /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/parameters.json\n",
      "Parameters(\n",
      "  base_dir = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon\n",
      "  data_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/data/parsed/five-three_5995.json\n",
      "  lexicon_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/data/parsed/lexicon_6788.json\n",
      "  output_dir = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary\n",
      "  output_targets_dir = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/targets\n",
      "  parameters_json_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/parameters.json\n",
      "  output_time_txt_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/elapsed_time.txt\n",
      "  output_pattern_evaluation_csv_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/sub/[%s]pattern_evaluation.csv\n",
      "  errlog_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/err/20200711-12-37-48.log\n",
      "  output_pattern_csv_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/sub/[%s]patterns_%d.csv\n",
      "  output_error_csv_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/sub/[%s]error_%d_%d.csv\n",
      "  output_target_log_csv_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/targets/%s_%02d_%s.csv\n",
      "  output_raw_df_pkl_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/save/raw_df.pkl\n",
      "  output_pattern_counter_pkl_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/save/[%s]pattern_counter.pkl\n",
      "  output_targets_concat_csv_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/save/[%s]targets.csv\n",
      "  num_cpus = 10\n",
      ")\n",
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import time, copy, os, pickle, glob, csv, ast\n",
    "from config import parameters\n",
    "from PatternHandler import PatternHandler\n",
    "from DependencyGraphHandler import DependencyGraphHandler\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers=parameters.num_cpus, progress_bar=True)\n",
    "\n",
    "data_filepath = parameters.data_filepath\n",
    "lexicon_filepath = parameters.lexicon_filepath\n",
    "output_time_txt_filepath = parameters.output_time_txt_filepath\n",
    "output_pattern_csv_filepath = parameters.output_pattern_csv_filepath\n",
    "output_error_csv_filepath = parameters.output_error_csv_filepath\n",
    "output_target_log_csv_filepath = parameters.output_target_log_csv_filepath\n",
    "output_raw_df_pkl_filepath = parameters.output_raw_df_pkl_filepath\n",
    "output_pattern_counter_pkl_filepath = parameters.output_pattern_counter_pkl_filepath\n",
    "output_targets_dir = parameters.output_targets_dir\n",
    "output_targets_concat_csv_filepath = parameters.output_targets_concat_csv_filepath\n",
    "output_pattern_evaluation_csv_filepath = parameters.output_pattern_evaluation_csv_filepath\n",
    "\n",
    "def match_opinion_words(content, opinion_word_lexicon):\n",
    "    opinion_words = []\n",
    "    for opinion in opinion_word_lexicon:\n",
    "        for token in content.split():\n",
    "            if token == opinion: opinion_words.append(token)\n",
    "    return list(set(opinion_words))\n",
    "\n",
    "def save_extracted_pattern_results(domain, pattern_counter, err_list):\n",
    "    pattern_list = [tup for tup in pattern_counter.items()]\n",
    "    pattern_df = pd.DataFrame(pattern_list, columns =['pattern', 'count'])  \n",
    "    filepath = output_pattern_csv_filepath % (domain, len(pattern_df))\n",
    "    pattern_df.to_csv(filepath, index = False, encoding='utf-8-sig')\n",
    "    print('Created %s' % filepath)\n",
    "    \n",
    "    err_df = pd.DataFrame(err_list, columns =['content', 'current_opinion_word', 'current_target_word', 'parse_error', 'opinion_words', 'targets', 'raw_targets'])  \n",
    "    filepath = output_error_csv_filepath % (domain, len(err_df[err_df['parse_error']==True]), len(err_df))\n",
    "    err_df.to_csv(filepath, index = False, encoding='utf-8-sig')\n",
    "    print('Created %s' % filepath)\n",
    "\n",
    "def pattern_extraction(domain, df, pattern_handler, dependency_handler):\n",
    "    pattern_counter, err_list = defaultdict(int), list()\n",
    "    pattern_handler.extract_patterns(df, pattern_counter, err_list, dependency_handler)\n",
    "    \n",
    "    save_extracted_pattern_results(domain, pattern_counter, err_list)\n",
    "    return pattern_counter\n",
    "\n",
    "def merge_dfs(data_filepaths):\n",
    "    dfs = []\n",
    "    for data_filepath in data_filepaths:\n",
    "        df = pd.read_csv(data_filepath)\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def calculate_true_positive(predicted_list, correct_list):\n",
    "    tp = 0\n",
    "    for predicted_compound_target in predicted_list:\n",
    "        if predicted_compound_target in correct_list:   # 'screen' <- predicted 'screen'\n",
    "            correct_list.remove(predicted_compound_target)\n",
    "            tp += 1\n",
    "            continue\n",
    "        for correct_target in correct_list:\n",
    "            if predicted_compound_target.find(correct_target) > -1:   # 'audio' <- predicted 'audio aspects'\n",
    "                correct_list.remove(correct_target)\n",
    "                tp += 1\n",
    "                break\n",
    "    return tp\n",
    "\n",
    "def calculate_precision_recall(df):\n",
    "    correct_targets_mul = list([item for sublist in df['targets'].values for item in sublist if item != ''])\n",
    "    predicted_targets_mul = list([item for sublist in df['predicted_targets'].values for item in sublist if item != ''])\n",
    "    tp_mul = calculate_true_positive(predicted_targets_mul, correct_targets_mul)\n",
    "    \n",
    "    if len(predicted_targets_mul) != 0: pre_mul = tp_mul / len(predicted_targets_mul)\n",
    "    else: pre_mul = 0\n",
    "    \n",
    "    if len(correct_targets_mul) != 0: rec_mul = tp_mul / len(correct_targets_mul)\n",
    "    else: rec_mul = 0\n",
    "    \n",
    "    correct_targets_dis = set([item for sublist in df['targets'].values for item in sublist if item != ''])\n",
    "    predicted_targets_dis = set([item for sublist in df['predicted_targets'].values for item in sublist if item != ''])\n",
    "    tp_dis = calculate_true_positive(predicted_targets_dis, correct_targets_dis)\n",
    "    \n",
    "    if len(predicted_targets_dis) != 0: pre_dis = tp_dis / len(predicted_targets_dis)\n",
    "    else: pre_dis = 0\n",
    "    \n",
    "    if len(correct_targets_dis) != 0: rec_dis = tp_dis / len(correct_targets_dis)\n",
    "    else: rec_dis = 0\n",
    "    \n",
    "    return pre_mul, rec_mul, pre_dis, rec_dis\n",
    "\n",
    "def calculate_f1(precision, recall):\n",
    "    denominator = precision + recall\n",
    "    if denominator == 0: return 0\n",
    "    return (2*precision*recall)/denominator\n",
    "\n",
    "def save_pkl(item_to_save, filepath):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(item_to_save, f)\n",
    "    print('Created %s' % filepath)\n",
    "\n",
    "def load_pkl(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        loaded_item = pickle.load(f)\n",
    "    print('Loaded %s' % filepath)\n",
    "    return loaded_item\n",
    "    \n",
    "def elapsed_time(start):\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    elapsed_time_txt = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "    text_file = open(output_time_txt_filepath, \"w\", encoding='utf-8')\n",
    "    content = 'Start: %s, End: %s => Elapsed time: %s\\nCreated %s' % (time.strftime(\"%H:%M:%S\", time.gmtime(start)), time.strftime(\"%H:%M:%S\", time.gmtime(end)), elapsed_time_txt, output_time_txt_filepath)\n",
    "    text_file.write(content)\n",
    "    text_file.close()\n",
    "    print('Created %s' % output_time_txt_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/dmlab/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/dmlab/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/home/dmlab/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/dmlab/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/home/dmlab/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/home/dmlab/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n",
      "Loaded /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/save/raw_df.pkl\n"
     ]
    }
   ],
   "source": [
    "pattern_handler = PatternHandler()\n",
    "dependency_handler = DependencyGraphHandler()\n",
    "\n",
    "if os.path.exists(output_raw_df_pkl_filepath): raw_df = load_pkl(output_raw_df_pkl_filepath)\n",
    "else:\n",
    "    raw_df = pd.read_json(data_filepath)\n",
    "    print('Matching opinion words..')\n",
    "    opinion_word_lexicon = [item for sublist in pd.read_json(lexicon_filepath).values for item in sublist]\n",
    "    raw_df['opinion_words'] = raw_df.parallel_apply(lambda x: match_opinion_words(x['content'], opinion_word_lexicon), axis=1)\n",
    "    print('Converting document into nlp(doc)..')\n",
    "    raw_df['doc'] = raw_df.progress_apply(lambda x: pattern_handler.nlp(x['content']), axis=1)\n",
    "\n",
    "    print('Filtering targets using nlp(doc)..')\n",
    "    raw_df['targets'] = raw_df.progress_apply(lambda x: pattern_handler.process_targets(x['content'], x['raw_targets']), axis=1) \n",
    "    save_pkl(raw_df, output_raw_df_pkl_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MP3 player', 'DVD player', 'Digital camera2', 'Cell phone',\n",
       "       'Digital camera1', 'Wireless router', 'Speaker', 'Computer'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df['domain'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wireless router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [Wireless router]..\n",
      "Loaded /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/save/[Wireless router]pattern_counter.pkl\n"
     ]
    }
   ],
   "source": [
    "domain = 'Wireless router'\n",
    "print('Processing [%s]..' % domain)\n",
    "df = raw_df[raw_df['domain']==domain]\n",
    "\n",
    "filepath = output_pattern_counter_pkl_filepath % domain\n",
    "if os.path.exists(filepath): pattern_counter = load_pkl(filepath)\n",
    "else: \n",
    "    pattern_counter = pattern_extraction(domain, df, pattern_handler, dependency_handler)\n",
    "    save_pkl(pattern_counter, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, stanfordnlp\n",
    "from nltk import pos_tag\n",
    "from DependencyGraph import DependencyGraph\n",
    "\n",
    "special_char_pattern = re.compile('([,.+]+.?\\d*)')\n",
    "nlp = pattern_handler.nlp\n",
    "noun = ['NN', 'NNS', 'NNP']\n",
    "doublespace_pattern = re.compile('\\s+')\n",
    "\n",
    "def leave_noun_only(term_list):\n",
    "    term_list = [doublespace_pattern.sub(' ', special_char_pattern.sub(' ', item)) for item in term_list if item != '']   # 'sound + quality'\n",
    "    term_list = [term for term, pos in pos_tag(term_list) if pos in noun and len(term) > 1]\n",
    "    return term_list\n",
    "    \n",
    "def extract_targets(predicted_targets, doc, opinion_words, dep_rels, dependency_handler):\n",
    "    if len(predicted_targets) > 0: \n",
    "        targets = predicted_targets\n",
    "    else: \n",
    "        targets = set()\n",
    "        for sentence_from_doc in doc.sentences:\n",
    "            sentence_graph = DependencyGraph(sentence_from_doc)\n",
    "            targets.update(dependency_handler.extract_targets_using_pattern(sentence_graph.token2idx, sentence_graph.nodes, opinion_words, dep_rels))\n",
    "\n",
    "        targets = list(targets)\n",
    "        targets = leave_noun_only(targets)\n",
    "        targets = list(set(targets))\n",
    "    #print(opinion_words, predicted_targets, '->', dep_rels, '=>', targets)\n",
    "    return targets\n",
    "\n",
    "def evaluate_rule_set(rule_set):\n",
    "    df['predicted_targets'] = df.apply(lambda x: list(), axis=1)\n",
    "    for one_flattened_dep_rels in rule_set:\n",
    "        dep_rels = one_flattened_dep_rels.split('-')\n",
    "        df['predicted_targets'] = df.apply(lambda x: extract_targets(x['predicted_targets'], x['doc'], x['opinion_words'], dep_rels, dependency_handler), axis=1)\n",
    "\n",
    "    pre_mul, rec_mul, pre_dis, rec_dis = calculate_precision_recall(df)\n",
    "    f1_mul = calculate_f1(pre_mul,rec_mul)\n",
    "    f1_dis = calculate_f1(pre_dis,rec_dis)\n",
    "    #print('\\nrule_set=%s'%str(rule_set) , 'f1_mul=%.2f'%calculate_f1(pre_mul,rec_mul), 'f1_dis=%.2f'%calculate_f1(pre_dis,rec_dis))\n",
    "    #print('pre_mul=%.2f'%pre_mul, 'rec_mul=%.2f'%rec_mul, 'pre_dis=%.2f'%pre_dis, 'rec_dis=%.2f'%rec_dis)\n",
    "    return f1_mul, f1_dis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DP를 적용하되 순서를 알맞게 고려하면 성능을 더 높일 수 있음   \n",
    "=> `amod nmod advmod rcmod nsubj obl => 0.63 0.43`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/dmlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amod nmod advmod rcmod nsubj obl => 0.63 0.43\n",
      "nsubj amod obl nmod advmod rcmod => 0.61 0.43\n"
     ]
    }
   ],
   "source": [
    "rules_ = ['amod', 'nmod', 'advmod', 'rcmod', 'nsubj', 'obl']\n",
    "print('%s => %.2f %.2f' % (' '.join(rules_), *evaluate_rule_set(rules_)))\n",
    "\n",
    "rules_ = ['nsubj', 'amod', 'obl', 'nmod', 'advmod', 'rcmod']\n",
    "print('%s => %.2f %.2f' % (' '.join(rules_), *evaluate_rule_set(rules_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1_mul 기준으로 랭킹 매긴 결과 참고 -> DP보다 높은 성능  \n",
    "`rule_set=['nsubj', 'amod', 'nsubj-compound-compound', 'obl'] f1_mul=0.65 f1_dis=0.47`   \n",
    "높은 성능을 내는 적절한 개수가 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/dmlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61 0.47\n",
      "0.62 0.48\n",
      "0.65 0.47\n",
      "0.65 0.47\n",
      "0.65 0.47\n"
     ]
    }
   ],
   "source": [
    "print('%.2f %.2f' % evaluate_rule_set(['nsubj', 'amod']))\n",
    "print('%.2f %.2f' % evaluate_rule_set(['nsubj', 'amod', 'nsubj-compound-compound']))\n",
    "print('%.2f %.2f' % evaluate_rule_set(['nsubj', 'amod', 'nsubj-compound-compound', 'obl']))\n",
    "print('%.2f %.2f' % evaluate_rule_set(['nsubj', 'amod', 'nsubj-compound-compound', 'obl', 'conj-conj-nsubj']))\n",
    "print('%.2f %.2f' % evaluate_rule_set(['nsubj', 'amod', 'nsubj-compound-compound', 'obl', 'conj-conj-nsubj', 'advcl-advcl-nsubj']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Possible combanitaions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import compress, product\n",
    "\n",
    "def get_combinations(items):\n",
    "    return ( set(compress(items,mask)) for mask in product(*[[0,1]]*len(items)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns = [pattern for pattern, count in pattern_counter.items() if count > 3]\n",
    "combinations = list(get_combinations(patterns))\n",
    "combinations.remove(set())\n",
    "len(combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`best_rule_set={'amod', 'conj-nsubj', 'amod-obj', 'nsubj', 'advmod-nsubj'}`   \n",
    "f1_mul 내림차순으로 하면 nsubj, amod, obl인데 obl은 best rule set에 뽑히지도 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0th..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/dmlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 50th..\n",
      "Processing 100th..\n",
      "Processing 150th..\n",
      "Processing 200th..\n",
      "Processing 250th..\n",
      "max_f1_mul=0.65 \t best_rule_set={'amod', 'conj-nsubj', 'amod-obj', 'nsubj', 'advmod-nsubj'}\n"
     ]
    }
   ],
   "source": [
    "max_f1_mul = 0\n",
    "best_rule_set = []\n",
    "rule_set_evaluation_dict = dict()\n",
    "for idx, comb in enumerate(combinations):\n",
    "    if idx % 50 == 0 : print('Processing %dth..' % idx)\n",
    "    f1_mul, f1_dis = evaluate_rule_set(comb)\n",
    "    rule_set_evaluation_dict[' '.join(comb)] = (f1_mul, f1_dis)\n",
    "    if f1_mul > max_f1_mul:\n",
    "        max_f1_mul = f1_mul\n",
    "        best_rule_set = comb\n",
    "print('max_f1_mul=%.2f \\t best_rule_set=%s' % (max_f1_mul, best_rule_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best rule set 안에서는 순서 상관 없음. redundant하지 않은 rule들이 뽑힌 것 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/dmlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63 0.50\n",
      "0.63 0.49\n",
      "0.63 0.49\n"
     ]
    }
   ],
   "source": [
    "print('%.2f %.2f' % evaluate_rule_set(['amod-obj', 'nsubj', 'conj-nsubj', 'amod', 'advmod-nsubj']))\n",
    "print('%.2f %.2f' % evaluate_rule_set(['nsubj', 'amod', 'conj-nsubj', 'advmod-nsubj', 'amod-obj']))\n",
    "print('%.2f %.2f' % evaluate_rule_set(['nsubj', 'amod', 'amod-obj', 'conj-nsubj', 'advmod-nsubj']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical data ~ mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nsubj', 23),\n",
       " ('conj-nsubj', 20),\n",
       " ('amod-obj', 11),\n",
       " ('amod', 10),\n",
       " ('obl', 7),\n",
       " ('advmod-nsubj', 7),\n",
       " ('amod-conj', 5),\n",
       " ('amod-obl', 4),\n",
       " ('xcomp-nsubj', 3),\n",
       " ('conj', 3),\n",
       " ('advmod', 3),\n",
       " ('amod-obj-nsubj', 2),\n",
       " ('amod-ccomp', 2),\n",
       " ('advmod-obl-conj-nsubj', 2),\n",
       " ('conj-obj', 2),\n",
       " ('conj-obl', 2),\n",
       " ('advmod-conj-nsubj', 2),\n",
       " ('obl-compound', 2),\n",
       " ('xcomp', 2),\n",
       " ('obj-nsubj', 2),\n",
       " ('conj-advcl', 2),\n",
       " ('ccomp-nsubj', 2),\n",
       " ('obl-nmod', 2),\n",
       " ('obj', 2),\n",
       " ('xcomp-acl-obj', 2),\n",
       " ('conj-nmod', 2),\n",
       " ('xcomp-obl', 2),\n",
       " ('xcomp-ccomp-obl-nmod', 2),\n",
       " ('appos-xcomp-ccomp-obl-nmod', 2),\n",
       " ('amod-conj-conj', 1),\n",
       " ('amod-obj-conj-nsubj-conj', 1),\n",
       " ('nsubj-det-obj', 1),\n",
       " ('punct-conj', 1),\n",
       " ('conj-nsubj-ccomp-xcomp', 1),\n",
       " ('xcomp-advcl-nsubj', 1),\n",
       " ('amod-obj-obl', 1),\n",
       " ('obj-obl', 1),\n",
       " ('mark-conj-nsubj', 1),\n",
       " ('parataxis-csubj-obj-nsubj', 1),\n",
       " ('obl-conj-nsubj', 1),\n",
       " ('conj-cc-conj-nsubj', 1),\n",
       " ('conj-conj-obj', 1),\n",
       " ('amod-compound-obl', 1),\n",
       " ('conj-obj-nmod', 1),\n",
       " ('conj-punct-conj', 1),\n",
       " ('obl-obl', 1),\n",
       " ('obl-obl-conj-compound', 1),\n",
       " ('nsubj:pass-obl-obl', 1),\n",
       " ('nsubj:pass-obl-obl-conj-compound', 1),\n",
       " ('amod-compound-compound-obl-obl', 1),\n",
       " ('amod-compound-compound-obl-obl-conj-compound', 1),\n",
       " ('amod-conj-nsubj:pass-obl-obl', 1),\n",
       " ('amod-conj-nsubj:pass-obl-obl-conj-compound', 1),\n",
       " ('conj-nsubj:pass', 1),\n",
       " ('conj-conj-nsubj:pass', 1),\n",
       " ('amod-obj-xcomp-conj-conj-nsubj:pass', 1),\n",
       " ('advmod-xcomp-nsubj', 1),\n",
       " ('advmod-conj-obj', 1),\n",
       " ('xcomp-conj', 1),\n",
       " ('advmod-xcomp', 1),\n",
       " ('xcomp-xcomp-obj', 1),\n",
       " ('obl-obj', 1),\n",
       " ('case-obl-obj', 1),\n",
       " ('advmod-acl:relcl-nsubj-parataxis-xcomp-obj', 1),\n",
       " ('amod-obl-conj-conj', 1),\n",
       " ('obj-conj-conj', 1),\n",
       " ('conj-conj', 1),\n",
       " ('vocative-punct-conj', 1),\n",
       " ('advmod-conj', 1),\n",
       " ('nmod-nmod', 1),\n",
       " ('amod-obj-acl-nsubj', 1),\n",
       " ('conj-csubj', 1),\n",
       " ('conj-advcl-obj', 1),\n",
       " ('advmod-conj-advcl-obj', 1),\n",
       " ('compound-obj', 1),\n",
       " ('mark-advcl-advcl-obl', 1),\n",
       " ('xcomp-advcl', 1),\n",
       " ('parataxis-nsubj', 1),\n",
       " ('obj-advcl', 1),\n",
       " ('parataxis-conj-nsubj', 1),\n",
       " ('amod-compound-appos-punct-compound', 1),\n",
       " ('amod-compound', 1),\n",
       " ('amod-obj-compound-appos-punct-compound', 1),\n",
       " ('amod-obj-compound', 1),\n",
       " ('obl-acl:relcl-compound', 1),\n",
       " ('compound-appos-punct-compound', 1),\n",
       " ('compound-appos-compound', 1),\n",
       " ('parataxis-obj-obl-acl:relcl-obl-compound', 1),\n",
       " ('parataxis-obj-obl-acl:relcl-compound', 1),\n",
       " ('acl-punct-ccomp', 1),\n",
       " ('xcomp-parataxis-ccomp', 1),\n",
       " ('conj-punct-ccomp', 1),\n",
       " ('obj-punct-ccomp', 1),\n",
       " ('advmod-conj-punct-ccomp', 1),\n",
       " ('obl-conj-ccomp', 1),\n",
       " ('conj-ccomp', 1),\n",
       " ('amod-obj-conj-nsubj', 1),\n",
       " ('csubj', 1),\n",
       " ('nsubj-ccomp', 1),\n",
       " ('amod-nsubj', 1),\n",
       " ('nsubj-ccomp-nsubj', 1),\n",
       " ('conj-cop-nsubj', 1),\n",
       " ('obl-punct-nsubj', 1),\n",
       " ('ccomp-acl:relcl-nsubj', 1),\n",
       " ('nmod', 1),\n",
       " ('advmod-obl', 1),\n",
       " ('advmod-obl-nsubj', 1),\n",
       " ('acl:relcl-obl:npmod-det-nsubj', 1),\n",
       " ('advcl-nsubj-compound', 1),\n",
       " ('advmod-conj-obj-conj', 1),\n",
       " ('obl-conj-obl', 1),\n",
       " ('nmod-obl-conj-obl', 1),\n",
       " ('acl-obj', 1),\n",
       " ('amod-obj-conj', 1),\n",
       " ('ccomp-obj', 1),\n",
       " ('amod-nmod-obj-nsubj', 1),\n",
       " ('nsubj-ccomp-advcl-obj', 1),\n",
       " ('amod-obl-mark-xcomp-obj', 1),\n",
       " ('nmod-obj-det', 1),\n",
       " ('nmod-obj-conj-obl-nmod', 1),\n",
       " ('amod-conj-obl-nmod', 1),\n",
       " ('parataxis-conj-nmod', 1),\n",
       " ('acl:relcl-obl-nsubj', 1),\n",
       " ('obj-nmod-acl-obj', 1),\n",
       " ('obj-nmod', 1),\n",
       " ('conj-advcl-obl', 1),\n",
       " ('advcl-obl', 1),\n",
       " ('acl:relcl-obl', 1),\n",
       " ('amod-parataxis', 1),\n",
       " ('obl-nsubj-parataxis', 1),\n",
       " ('case-obl-nsubj-parataxis', 1),\n",
       " ('appos-xcomp-conj-obl', 1),\n",
       " ('xcomp-conj-obl', 1),\n",
       " ('advcl-obj', 1),\n",
       " ('xcomp-acl-obj-xcomp-conj-nsubj', 1),\n",
       " ('amod-obj-acl-obj', 1),\n",
       " ('amod-obj-xcomp-conj-nsubj', 1),\n",
       " ('xcomp-conj-xcomp-obj-acl-obj', 1),\n",
       " ('parataxis', 1),\n",
       " ('xcomp-conj-nsubj', 1),\n",
       " ('ccomp-cop', 1),\n",
       " ('conj-obl-compound', 1),\n",
       " ('amod-obl-csubj', 1),\n",
       " ('amod-nmod-conj-cc-conj-obj', 1),\n",
       " ('obj-advcl-obj', 1),\n",
       " ('amod-nmod-case-nmod', 1),\n",
       " ('amod-nmod', 1),\n",
       " ('obl-nmod-case-nmod', 1),\n",
       " ('nmod-conj-nsubj', 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(pattern_counter.items(), key=lambda x: x[-1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>raw_targets</th>\n",
       "      <th>filename</th>\n",
       "      <th>domain</th>\n",
       "      <th>opinion_words</th>\n",
       "      <th>doc</th>\n",
       "      <th>targets</th>\n",
       "      <th>predicted_targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>Affordable price , reliable item and excellen...</td>\n",
       "      <td>[price, item, customer service]</td>\n",
       "      <td>Router</td>\n",
       "      <td>Wireless router</td>\n",
       "      <td>[reliable, excellent]</td>\n",
       "      <td>&lt;stanfordnlp.pipeline.doc.Document object at 0...</td>\n",
       "      <td>[price, customer service, item]</td>\n",
       "      <td>[customer service, item]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>I did have problem connecting my laptop to th...</td>\n",
       "      <td>[customer service]</td>\n",
       "      <td>Router</td>\n",
       "      <td>Wireless router</td>\n",
       "      <td>[problem, patient, helpful]</td>\n",
       "      <td>&lt;stanfordnlp.pipeline.doc.Document object at 0...</td>\n",
       "      <td>[customer service]</td>\n",
       "      <td>[customer representative]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>I have no regrets and thankful to find this p...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Router</td>\n",
       "      <td>Wireless router</td>\n",
       "      <td>[thankful, regrets]</td>\n",
       "      <td>&lt;stanfordnlp.pipeline.doc.Document object at 0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>After much homework and too much frustration ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Router</td>\n",
       "      <td>Wireless router</td>\n",
       "      <td>[frustration]</td>\n",
       "      <td>&lt;stanfordnlp.pipeline.doc.Document object at 0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3949</th>\n",
       "      <td>I am above average with technical knowledge b...</td>\n",
       "      <td>[set up]</td>\n",
       "      <td>Router</td>\n",
       "      <td>Wireless router</td>\n",
       "      <td>[enough, easy]</td>\n",
       "      <td>&lt;stanfordnlp.pipeline.doc.Document object at 0...</td>\n",
       "      <td>[set up]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  \\\n",
       "3945   Affordable price , reliable item and excellen...   \n",
       "3946   I did have problem connecting my laptop to th...   \n",
       "3947   I have no regrets and thankful to find this p...   \n",
       "3948   After much homework and too much frustration ...   \n",
       "3949   I am above average with technical knowledge b...   \n",
       "\n",
       "                          raw_targets filename           domain  \\\n",
       "3945  [price, item, customer service]   Router  Wireless router   \n",
       "3946               [customer service]   Router  Wireless router   \n",
       "3947                               []   Router  Wireless router   \n",
       "3948                               []   Router  Wireless router   \n",
       "3949                         [set up]   Router  Wireless router   \n",
       "\n",
       "                    opinion_words  \\\n",
       "3945        [reliable, excellent]   \n",
       "3946  [problem, patient, helpful]   \n",
       "3947          [thankful, regrets]   \n",
       "3948                [frustration]   \n",
       "3949               [enough, easy]   \n",
       "\n",
       "                                                    doc  \\\n",
       "3945  <stanfordnlp.pipeline.doc.Document object at 0...   \n",
       "3946  <stanfordnlp.pipeline.doc.Document object at 0...   \n",
       "3947  <stanfordnlp.pipeline.doc.Document object at 0...   \n",
       "3948  <stanfordnlp.pipeline.doc.Document object at 0...   \n",
       "3949  <stanfordnlp.pipeline.doc.Document object at 0...   \n",
       "\n",
       "                              targets          predicted_targets  \n",
       "3945  [price, customer service, item]   [customer service, item]  \n",
       "3946               [customer service]  [customer representative]  \n",
       "3947                               []                         []  \n",
       "3948                               []                         []  \n",
       "3949                         [set up]                         []  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97 97 22]\n",
      "['acl:relcl-obl', 'acl:relcl-obl', 'acl-punct-ccomp']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(list(pattern_counter.keys()))\n",
    "# print(list(le.classes_))\n",
    "print(le.transform([\"nsubj\", \"nsubj\", \"amod\"]))\n",
    "print(list(le.inverse_transform([2, 2, 1])))\n",
    "\n",
    "#x1=x.apply(LabelEncoder().fit_transform)\n",
    "#mutual_info_classif(x1, y, discrete_features=[1, 2, 3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

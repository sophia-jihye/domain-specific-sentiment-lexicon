{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file: /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/parameters.json\n",
      "Parameters(\n",
      "  base_dir = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon\n",
      "  data_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/data/parsed/five-three_5995.json\n",
      "  lexicon_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/data/parsed/lexicon_6788.json\n",
      "  output_dir = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary\n",
      "  output_targets_dir = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/targets\n",
      "  parameters_json_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/parameters.json\n",
      "  output_time_txt_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/elapsed_time.txt\n",
      "  output_pattern_evaluation_csv_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/[%s]pattern_evaluation.csv\n",
      "  errlog_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/err/20200710-17-08-24.log\n",
      "  output_pattern_csv_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/sub/[%s]patterns_%d.csv\n",
      "  output_error_csv_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/sub/[%s]error_%d_%d.csv\n",
      "  output_target_log_csv_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/targets/%s_%02d_%s.csv\n",
      "  output_raw_df_pkl_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/save/raw_df.pkl\n",
      "  output_pattern_counter_pkl_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/save/[%s]pattern_counter.pkl\n",
      "  output_targets_concat_csv_filepath = /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/save/[%s]targets.csv\n",
      "  num_cpus = 10\n",
      ")\n",
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import time, copy, os, pickle, glob, csv, ast\n",
    "from config import parameters\n",
    "from PatternHandler import PatternHandler\n",
    "from DependencyGraphHandler import DependencyGraphHandler\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers=parameters.num_cpus, progress_bar=True)\n",
    "\n",
    "data_filepath = parameters.data_filepath\n",
    "lexicon_filepath = parameters.lexicon_filepath\n",
    "output_time_txt_filepath = parameters.output_time_txt_filepath\n",
    "output_pattern_csv_filepath = parameters.output_pattern_csv_filepath\n",
    "output_error_csv_filepath = parameters.output_error_csv_filepath\n",
    "output_target_log_csv_filepath = parameters.output_target_log_csv_filepath\n",
    "output_raw_df_pkl_filepath = parameters.output_raw_df_pkl_filepath\n",
    "output_pattern_counter_pkl_filepath = parameters.output_pattern_counter_pkl_filepath\n",
    "output_targets_dir = parameters.output_targets_dir\n",
    "output_targets_concat_csv_filepath = parameters.output_targets_concat_csv_filepath\n",
    "output_pattern_evaluation_csv_filepath = parameters.output_pattern_evaluation_csv_filepath\n",
    "\n",
    "def match_opinion_words(content, opinion_word_lexicon):\n",
    "    opinion_words = []\n",
    "    for opinion in opinion_word_lexicon:\n",
    "        for token in content.split():\n",
    "            if token == opinion: opinion_words.append(token)\n",
    "    return list(set(opinion_words))\n",
    "\n",
    "def save_extracted_pattern_results(domain, pattern_counter, err_list):\n",
    "    pattern_list = [tup for tup in pattern_counter.items()]\n",
    "    pattern_df = pd.DataFrame(pattern_list, columns =['pattern', 'count'])  \n",
    "    filepath = output_pattern_csv_filepath % (domain, len(pattern_df))\n",
    "    pattern_df.to_csv(filepath, index = False, encoding='utf-8-sig')\n",
    "    print('Created %s' % filepath)\n",
    "    \n",
    "    err_df = pd.DataFrame(err_list, columns =['content', 'current_opinion_word', 'current_target_word', 'parse_error', 'opinion_words', 'targets', 'raw_targets'])  \n",
    "    filepath = output_error_csv_filepath % (domain, len(err_df[err_df['parse_error']==True]), len(err_df))\n",
    "    err_df.to_csv(filepath, index = False, encoding='utf-8-sig')\n",
    "    print('Created %s' % filepath)\n",
    "\n",
    "def pattern_extraction(domain, df, pattern_handler, dependency_handler):\n",
    "    pattern_counter, err_list = defaultdict(int), list()\n",
    "    pattern_handler.extract_patterns(df, pattern_counter, err_list, dependency_handler)\n",
    "    \n",
    "    save_extracted_pattern_results(domain, pattern_counter, err_list)\n",
    "    return pattern_counter\n",
    "\n",
    "def merge_dfs(data_filepaths):\n",
    "    dfs = []\n",
    "    for data_filepath in data_filepaths:\n",
    "        df = pd.read_csv(data_filepath)\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def calculate_true_positive(predicted_list, correct_list):\n",
    "    tp = 0\n",
    "    for predicted_compound_target in predicted_list:\n",
    "        if predicted_compound_target in correct_list:   # 'screen' <- predicted 'screen'\n",
    "            correct_list.remove(predicted_compound_target)\n",
    "            tp += 1\n",
    "            continue\n",
    "        for correct_target in correct_list:\n",
    "            if predicted_compound_target.find(correct_target) > -1:   # 'audio' <- predicted 'audio aspects'\n",
    "                correct_list.remove(correct_target)\n",
    "                tp += 1\n",
    "                break\n",
    "    return tp\n",
    "\n",
    "def calculate_precision_recall(df):\n",
    "    correct_targets_mul = list([item for sublist in df['targets'].values for item in sublist if item != ''])\n",
    "    predicted_targets_mul = list([item for sublist in df['predicted_targets'].values for item in sublist if item != ''])\n",
    "    tp_mul = calculate_true_positive(predicted_targets_mul, correct_targets_mul)\n",
    "    \n",
    "    if len(predicted_targets_mul) != 0: pre_mul = tp_mul / len(predicted_targets_mul)\n",
    "    else: pre_mul = 0\n",
    "    \n",
    "    if len(correct_targets_mul) != 0: rec_mul = tp_mul / len(correct_targets_mul)\n",
    "    else: rec_mul = 0\n",
    "    \n",
    "    correct_targets_dis = set([item for sublist in df['targets'].values for item in sublist if item != ''])\n",
    "    predicted_targets_dis = set([item for sublist in df['predicted_targets'].values for item in sublist if item != ''])\n",
    "    tp_dis = calculate_true_positive(predicted_targets_dis, correct_targets_dis)\n",
    "    \n",
    "    if len(predicted_targets_dis) != 0: pre_dis = tp_dis / len(predicted_targets_dis)\n",
    "    else: pre_dis = 0\n",
    "    \n",
    "    if len(correct_targets_dis) != 0: rec_dis = tp_dis / len(correct_targets_dis)\n",
    "    else: rec_dis = 0\n",
    "    \n",
    "    return pre_mul, rec_mul, pre_dis, rec_dis\n",
    "\n",
    "def calculate_f1(precision, recall):\n",
    "    denominator = precision + recall\n",
    "    if denominator == 0: return 0\n",
    "    return (2*precision*recall)/denominator\n",
    "\n",
    "def save_pkl(item_to_save, filepath):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(item_to_save, f)\n",
    "    print('Created %s' % filepath)\n",
    "\n",
    "def load_pkl(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        loaded_item = pickle.load(f)\n",
    "    print('Loaded %s' % filepath)\n",
    "    return loaded_item\n",
    "    \n",
    "def elapsed_time(start):\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    elapsed_time_txt = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "    text_file = open(output_time_txt_filepath, \"w\", encoding='utf-8')\n",
    "    content = 'Start: %s, End: %s => Elapsed time: %s\\nCreated %s' % (time.strftime(\"%H:%M:%S\", time.gmtime(start)), time.strftime(\"%H:%M:%S\", time.gmtime(end)), elapsed_time_txt, output_time_txt_filepath)\n",
    "    text_file.write(content)\n",
    "    text_file.close()\n",
    "    print('Created %s' % output_time_txt_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/dmlab/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/dmlab/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/home/dmlab/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/dmlab/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/home/dmlab/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/home/dmlab/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n",
      "Loaded /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/save/raw_df.pkl\n"
     ]
    }
   ],
   "source": [
    "pattern_handler = PatternHandler()\n",
    "dependency_handler = DependencyGraphHandler()\n",
    "\n",
    "if os.path.exists(output_raw_df_pkl_filepath): raw_df = load_pkl(output_raw_df_pkl_filepath)\n",
    "else:\n",
    "    raw_df = pd.read_json(data_filepath)\n",
    "    print('Matching opinion words..')\n",
    "    opinion_word_lexicon = [item for sublist in pd.read_json(lexicon_filepath).values for item in sublist]\n",
    "    raw_df['opinion_words'] = raw_df.parallel_apply(lambda x: match_opinion_words(x['content'], opinion_word_lexicon), axis=1)\n",
    "    print('Converting document into nlp(doc)..')\n",
    "    raw_df['doc'] = raw_df.progress_apply(lambda x: pattern_handler.nlp(x['content']), axis=1)\n",
    "\n",
    "    print('Filtering targets using nlp(doc)..')\n",
    "    raw_df['targets'] = raw_df.progress_apply(lambda x: pattern_handler.process_targets(x['content'], x['raw_targets']), axis=1) \n",
    "    save_pkl(raw_df, output_raw_df_pkl_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MP3 player', 'DVD player', 'Digital camera2', 'Cell phone',\n",
       "       'Digital camera1', 'Wireless router', 'Speaker', 'Computer'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df['domain'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell phone domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [Cell phone]..\n",
      "Loaded /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/save/[Cell phone]pattern_counter.pkl\n"
     ]
    }
   ],
   "source": [
    "domain = 'Cell phone'\n",
    "print('Processing [%s]..' % domain)\n",
    "df = raw_df[raw_df['domain']==domain]\n",
    "\n",
    "filepath = output_pattern_counter_pkl_filepath % domain\n",
    "if os.path.exists(filepath): pattern_counter = load_pkl(filepath)\n",
    "else: \n",
    "    pattern_counter = pattern_extraction(domain, df, pattern_handler, dependency_handler)\n",
    "    save_pkl(pattern_counter, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, stanfordnlp\n",
    "from DependencyGraph import DependencyGraph\n",
    "\n",
    "special_char_pattern = re.compile('([,.+]+.?\\d*)')\n",
    "nlp = pattern_handler.nlp\n",
    "noun = ['NN', 'NNS', 'NNP']\n",
    "doublespace_pattern = re.compile('\\s+')\n",
    "\n",
    "def leave_noun_only(term_list):\n",
    "    term_list = [doublespace_pattern.sub(' ', special_char_pattern.sub(' ', item)) for item in term_list if item != '']   # 'sound + quality'\n",
    "    term_list = [term for term, pos in pos_tag(term_list) if pos in noun and len(term) > 1]\n",
    "    return term_list\n",
    "    \n",
    "def extract_targets(predicted_targets, doc, opinion_words, dep_rels, dependency_handler):\n",
    "    if len(predicted_targets) > 0: \n",
    "        targets = predicted_targets\n",
    "    else: \n",
    "        targets = set()\n",
    "        for sentence_from_doc in doc.sentences:\n",
    "            sentence_graph = DependencyGraph(sentence_from_doc)\n",
    "            targets.update(dependency_handler.extract_targets_using_pattern(sentence_graph.token2idx, sentence_graph.nodes, opinion_words, dep_rels))\n",
    "\n",
    "        targets = list(targets)\n",
    "        targets = leave_noun_only(targets)\n",
    "        targets = list(set(targets))\n",
    "    #print(opinion_words, predicted_targets, '->', dep_rels, '=>', targets)\n",
    "    return targets\n",
    "\n",
    "def evaluate_rule_set(rule_set):\n",
    "    df['predicted_targets'] = df.apply(lambda x: list(), axis=1)\n",
    "    for one_flattened_dep_rels in rule_set:\n",
    "        dep_rels = one_flattened_dep_rels.split('-')\n",
    "        df['predicted_targets'] = df.apply(lambda x: extract_targets(x['predicted_targets'], x['doc'], x['opinion_words'], dep_rels, dependency_handler), axis=1)\n",
    "\n",
    "    pre_mul, rec_mul, pre_dis, rec_dis = calculate_precision_recall(df)\n",
    "    f1_mul = calculate_f1(pre_mul,rec_mul)\n",
    "    f1_dis = calculate_f1(pre_dis,rec_dis)\n",
    "    print('\\nrule_set=%s'%str(rule_set) , 'f1_mul=%.2f'%calculate_f1(pre_mul,rec_mul), 'f1_dis=%.2f'%calculate_f1(pre_dis,rec_dis))\n",
    "    print('pre_mul=%.2f'%pre_mul, 'rec_mul=%.2f'%rec_mul, 'pre_dis=%.2f'%pre_dis, 'rec_dis=%.2f'%rec_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DP를 적용하되 순서를 알맞게 고려하면 성능을 더 높일 수 있음   \n",
    "=> `rule_set=['nsubj', 'amod', 'obl', 'nmod', 'advmod', 'rcmod'] f1_mul=0.89 f1_dis=0.64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/dmlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "rule_set=['amod', 'nmod', 'advmod', 'rcmod', 'nsubj', 'obl'] f1_mul=0.85 f1_dis=0.59\n",
      "pre_mul=0.57 rec_mul=1.68 pre_dis=0.35 rec_dis=1.78\n",
      "\n",
      "rule_set=['nsubj', 'amod', 'obl', 'nmod', 'advmod', 'rcmod'] f1_mul=0.89 f1_dis=0.64\n",
      "pre_mul=0.58 rec_mul=1.88 pre_dis=0.38 rec_dis=2.03\n"
     ]
    }
   ],
   "source": [
    "evaluate_rule_set(['amod', 'nmod', 'advmod', 'rcmod', 'nsubj', 'obl'])\n",
    "evaluate_rule_set(['nsubj', 'amod', 'obl', 'nmod', 'advmod', 'rcmod'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1_mul 기준으로 랭킹 매긴 결과 참고 -> DP보다 높은 성능  \n",
    "`rule_set=['nsubj', 'amod', 'nsubj-compound-compound'] f1_mul=0.90 f1_dis=0.69`   \n",
    "높은 성능을 내는 적절한 개수가 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/dmlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "rule_set=['nsubj', 'amod'] f1_mul=0.89 f1_dis=0.69\n",
      "pre_mul=0.64 rec_mul=1.46 pre_dis=0.44 rec_dis=1.63\n",
      "\n",
      "rule_set=['nsubj', 'amod', 'nsubj-compound-compound'] f1_mul=0.90 f1_dis=0.69\n",
      "pre_mul=0.65 rec_mul=1.48 pre_dis=0.44 rec_dis=1.63\n",
      "\n",
      "rule_set=['nsubj', 'amod', 'nsubj-compound-compound', 'conj-conj-nsubj'] f1_mul=0.90 f1_dis=0.69\n",
      "pre_mul=0.65 rec_mul=1.48 pre_dis=0.44 rec_dis=1.63\n",
      "\n",
      "rule_set=['nsubj', 'amod', 'nsubj-compound-compound', 'conj-conj-nsubj', 'advcl-advcl-nsubj'] f1_mul=0.90 f1_dis=0.69\n",
      "pre_mul=0.64 rec_mul=1.48 pre_dis=0.44 rec_dis=1.63\n"
     ]
    }
   ],
   "source": [
    "evaluate_rule_set(['nsubj', 'amod'])\n",
    "evaluate_rule_set(['nsubj', 'amod', 'nsubj-compound-compound'])\n",
    "evaluate_rule_set(['nsubj', 'amod', 'nsubj-compound-compound', 'conj-conj-nsubj'])\n",
    "evaluate_rule_set(['nsubj', 'amod', 'nsubj-compound-compound', 'conj-conj-nsubj', 'advcl-advcl-nsubj'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wireless router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [Wireless router]..\n",
      "Loaded /home/dmlab/jihye/GIT/domain-specific-sentiment-lexicon/output/preliminary/save/[Wireless router]pattern_counter.pkl\n"
     ]
    }
   ],
   "source": [
    "domain = 'Wireless router'\n",
    "print('Processing [%s]..' % domain)\n",
    "df = raw_df[raw_df['domain']==domain]\n",
    "\n",
    "filepath = output_pattern_counter_pkl_filepath % domain\n",
    "if os.path.exists(filepath): pattern_counter = load_pkl(filepath)\n",
    "else: \n",
    "    pattern_counter = pattern_extraction(domain, df, pattern_handler, dependency_handler)\n",
    "    save_pkl(pattern_counter, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, stanfordnlp\n",
    "from DependencyGraph import DependencyGraph\n",
    "\n",
    "special_char_pattern = re.compile('([,.+]+.?\\d*)')\n",
    "nlp = pattern_handler.nlp\n",
    "noun = ['NN', 'NNS', 'NNP']\n",
    "doublespace_pattern = re.compile('\\s+')\n",
    "\n",
    "def leave_noun_only(term_list):\n",
    "    term_list = [doublespace_pattern.sub(' ', special_char_pattern.sub(' ', item)) for item in term_list if item != '']   # 'sound + quality'\n",
    "    term_list = [term for term, pos in pos_tag(term_list) if pos in noun and len(term) > 1]\n",
    "    return term_list\n",
    "    \n",
    "def extract_targets(predicted_targets, doc, opinion_words, dep_rels, dependency_handler):\n",
    "    if len(predicted_targets) > 0: \n",
    "        targets = predicted_targets\n",
    "    else: \n",
    "        targets = set()\n",
    "        for sentence_from_doc in doc.sentences:\n",
    "            sentence_graph = DependencyGraph(sentence_from_doc)\n",
    "            targets.update(dependency_handler.extract_targets_using_pattern(sentence_graph.token2idx, sentence_graph.nodes, opinion_words, dep_rels))\n",
    "\n",
    "        targets = list(targets)\n",
    "        targets = leave_noun_only(targets)\n",
    "        targets = list(set(targets))\n",
    "    #print(opinion_words, predicted_targets, '->', dep_rels, '=>', targets)\n",
    "    return targets\n",
    "\n",
    "def evaluate_rule_set(rule_set):\n",
    "    df['predicted_targets'] = df.apply(lambda x: list(), axis=1)\n",
    "    for one_flattened_dep_rels in rule_set:\n",
    "        dep_rels = one_flattened_dep_rels.split('-')\n",
    "        df['predicted_targets'] = df.apply(lambda x: extract_targets(x['predicted_targets'], x['doc'], x['opinion_words'], dep_rels, dependency_handler), axis=1)\n",
    "\n",
    "    pre_mul, rec_mul, pre_dis, rec_dis = calculate_precision_recall(df)\n",
    "    f1_mul = calculate_f1(pre_mul,rec_mul)\n",
    "    f1_dis = calculate_f1(pre_dis,rec_dis)\n",
    "    print('\\nrule_set=%s'%str(rule_set) , 'f1_mul=%.2f'%calculate_f1(pre_mul,rec_mul), 'f1_dis=%.2f'%calculate_f1(pre_dis,rec_dis))\n",
    "    print('pre_mul=%.2f'%pre_mul, 'rec_mul=%.2f'%rec_mul, 'pre_dis=%.2f'%pre_dis, 'rec_dis=%.2f'%rec_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DP를 적용하되 순서를 알맞게 고려하면 성능을 더 높일 수 있음   \n",
    "=> `rule_set=['amod', 'nmod', 'advmod', 'rcmod', 'nsubj', 'obl'] f1_mul=0.62 f1_dis=0.43`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/dmlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "rule_set=['amod', 'nmod', 'advmod', 'rcmod', 'nsubj', 'obl'] f1_mul=0.62 f1_dis=0.43\n",
      "pre_mul=0.43 rec_mul=1.12 pre_dis=0.28 rec_dis=0.94\n",
      "\n",
      "rule_set=['nsubj', 'amod', 'obl', 'nmod', 'advmod', 'rcmod'] f1_mul=0.61 f1_dis=0.42\n",
      "pre_mul=0.42 rec_mul=1.09 pre_dis=0.28 rec_dis=0.92\n"
     ]
    }
   ],
   "source": [
    "evaluate_rule_set(['amod', 'nmod', 'advmod', 'rcmod', 'nsubj', 'obl'])\n",
    "evaluate_rule_set(['nsubj', 'amod', 'obl', 'nmod', 'advmod', 'rcmod'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1_mul 기준으로 랭킹 매긴 결과 참고 -> DP보다 높은 성능  \n",
    "`rule_set=['nsubj', 'amod', 'nsubj-compound-compound', 'obl'] f1_mul=0.64 f1_dis=0.46`   \n",
    "높은 성능을 내는 적절한 개수가 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/dmlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "rule_set=['nsubj', 'amod'] f1_mul=0.61 f1_dis=0.47\n",
      "pre_mul=0.51 rec_mul=0.77 pre_dis=0.35 rec_dis=0.72\n",
      "\n",
      "rule_set=['nsubj', 'amod', 'nsubj-compound-compound'] f1_mul=0.62 f1_dis=0.48\n",
      "pre_mul=0.52 rec_mul=0.78 pre_dis=0.36 rec_dis=0.74\n",
      "\n",
      "rule_set=['nsubj', 'amod', 'nsubj-compound-compound', 'obl'] f1_mul=0.64 f1_dis=0.46\n",
      "pre_mul=0.47 rec_mul=1.00 pre_dis=0.31 rec_dis=0.89\n",
      "\n",
      "rule_set=['nsubj', 'amod', 'nsubj-compound-compound', 'obl', 'conj-conj-nsubj'] f1_mul=0.64 f1_dis=0.46\n",
      "pre_mul=0.47 rec_mul=1.00 pre_dis=0.31 rec_dis=0.89\n",
      "\n",
      "rule_set=['nsubj', 'amod', 'nsubj-compound-compound', 'obl', 'conj-conj-nsubj', 'advcl-advcl-nsubj'] f1_mul=0.64 f1_dis=0.46\n",
      "pre_mul=0.47 rec_mul=1.00 pre_dis=0.31 rec_dis=0.89\n"
     ]
    }
   ],
   "source": [
    "evaluate_rule_set(['nsubj', 'amod'])\n",
    "evaluate_rule_set(['nsubj', 'amod', 'nsubj-compound-compound'])\n",
    "evaluate_rule_set(['nsubj', 'amod', 'nsubj-compound-compound', 'obl'])\n",
    "evaluate_rule_set(['nsubj', 'amod', 'nsubj-compound-compound', 'obl', 'conj-conj-nsubj'])\n",
    "evaluate_rule_set(['nsubj', 'amod', 'nsubj-compound-compound', 'obl', 'conj-conj-nsubj', 'advcl-advcl-nsubj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "state": {
    "0bd9750a292b414591d18f194ba45410": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    },
    "1c92a4b3876244438690f47abc3498a3": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    },
    "75346dbf36d0451e9da1b0b75746b489": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    },
    "c87297beef3449bca68e6309a6b77f11": {
     "views": [
      {
       "cell_index": 3
      }
     ]
    },
    "d8ca1f6585884dca867a134e8458c7dd": {
     "views": [
      {
       "cell_index": 3
      }
     ]
    },
    "dc94422084b24dd1b826f66a0e383718": {
     "views": [
      {
       "cell_index": 3
      }
     ]
    },
    "ee8bc4685d5044af935501faeb85460f": {
     "views": [
      {
       "cell_index": 3
      }
     ]
    },
    "f20001ed74ca43bc9b939a847c7eaf56": {
     "views": [
      {
       "cell_index": 3
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
